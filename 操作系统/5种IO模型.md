# 5种IO模型

以两个应用程序通讯为例，我们来了解一下当“A”向"B" 发送一条消息时的流程：

**第一步**：应用A把消息发送到TCP发送缓冲区

**第二步**：TCP发送缓冲区再把消息发送出去，经过网络传递后，消息会发送到B服务器的TCP接收缓冲区

**第三步**：B再从TCP接收缓冲区去读取属于自己的数据

<img src="https://raw.githubusercontent.com/KKKLxxx/img-host/master/202112012252040.jpeg" alt="img" style="zoom:80%;" />

## 一、阻塞IO（BIO, Blocking IO）

阻塞IO即，在应用调用recvfrom读取数据时，其系统调用直到数据包到达且被复制到应用缓冲区中或者发送错误时才返回，在此期间一直会等待，进程从调用到返回这段时间内都是被阻塞的，所以称为阻塞IO

## 二、非阻塞IO（NIO, Non-Blocking IO）

非阻塞IO即，在应用调用recvfrom读取数据时，如果该缓冲区没有数据的话，就会直接返回一个错误，不会让应用一直等待，不过应用需要不断的调用recvfrom请求，直到读取到它数据要的数据为止

## 三、IO多路复用

**非阻塞IO的缺陷**：如果同时有N个人向应用发送消息，应用就必须创建多个线程调用recvfrom去读取数据。同时又因为应用线程不知道什么时候会有数据读取，为了保证消息能及时读取到，这些线程又必须不断地向内核发送recvfrom请求。这样造成了很大的资源浪费

IO多路复用提供了一个解决办法：**用一个线程监控多个网络请求，这些网络请求用fd（文件描述符）来标识。进程将fd传递给一种用于监测fd状态的函数，当有数据准备就绪时，这个函数返回可读状态，再调用真正需要处理数据的线程去读取数据**

用于监测fd状态的函数有3种：select、poll、epoll

### 1、select

首先将用户态的fd以数组的方式复制到内核，select函数重复遍历fd，直到有fd可读。然后将fd复制回用户态，即可读取数据

显然，select有3个缺陷

1、每次调用select，都要将所有fd复制到内核，资源消耗会随着fd数量的增加而增加

2、因为底层结构是数组，其监听端口数大小受到了一个名为FD_SETSIZE的宏定义的限制，默认1024

3、当有fd可读时，是将所有fd复制回用户态。但并不知道具体哪个fd可读，所以又需要遍历一遍

### 2、poll

poll相比select只有一点改进，即poll的底层结构变为了链表，所以没有了监听数量的限制

其他两个性能问题仍然存在

### 3、epoll

为了解决性能问题，epoll主要做出2个改进：

1、在**内核态**，通过**红黑树**实现对fd节点的快速查找、插入、删除

2、每当有fd就绪，就可以通过一个回调函数，将fd传回**用户态**，并将其存储在一个**链表**中，不需要再遍历所有fd

<img src="https://raw.githubusercontent.com/KKKLxxx/img-host/master/epoll.jpg" alt="epoll" style="zoom: 67%;" />

epoll有两个关键的函数：

- **epoll_ctl()**：将一个fd添加到内核态的红黑树中
- **epoll_wait()**：检查用户态的链表是否为空，不为空则返回已就绪的fd数量

### 4、水平触发与边缘触发

1、水平触发（Level-Trigger）：**只要缓冲区中有数据，就会被触发**。因为未处理的数据会持续通知，所以编程更简单

2、边缘触发（Edge-Trigger）：**只有当缓冲区由空转化为非空时，才会被触发**。但是为了保证数据的完整性，需要一次性处理完缓冲区的数据，否则可能发生遗漏，所以编程相对复杂

如果使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要一次执行尽可能多的读写操作

如果使用边缘触发模式，IO事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会循环从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，**边缘触发模式一般和非阻塞IO搭配使用**，程序会一直执行IO操作，直到系统调用（如 `read` 和 `write`）返回错误

一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 epoll_wait() 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换

### 5、对比

|                | select             | poll               | epoll                             |
| -------------- | ------------------ | ------------------ | --------------------------------- |
| **性能**       | 随连接数增加而降低 | 随连接数增加而降低 | 几乎不受连接数影响                |
| **连接数**     | 默认1024           | 无限制             | 无限制                            |
| **底层结构**   | 数组               | 链表               | 红黑树+链表                       |
| **内存拷贝**   | 要拷贝所有fd       | 要拷贝所有fd       | 调用ctl需要拷贝，调用wait不用拷贝 |
| **处理方式**   | 轮询               | 轮询               | 回调                              |
| **触发模式**   | 水平触发           | 水平触发           | 水平触发/边缘触发                 |
| **时间复杂度** | O(n)               | O(n)               | O(1)                              |

### 6、Reactor模式

Reactor模式是**一种基于IO多路复用的事件处理模式**

Reactor模式也叫Dispatcher模式，即IO多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个线程

Reactor模式**主要由Reactor和处理资源池这两个核心部分组成**，它俩负责的事情如下：

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send

根据Reactor与资源池中线程数量的不同，可组成以下3种实用的组合

- 单Reactor单线程：无法充分利用多核CPU性能

- 单Reactor多线程：可以利用多核CPU性能，但Reactor可能称为性能瓶颈

- 多Reactor多线程：整体架构如下图

  <img src="https://raw.githubusercontent.com/KKKLxxx/img-host/master/%E4%B8%BB%E4%BB%8EReactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.jpg" alt="主从Reactor多线程" style="zoom: 50%;" />

## 四、信号驱动IO

信号驱动IO通过一个信号建立线程与fd的关系，当fd就绪时，就传回一个信号通知线程可以读取数据

信号驱动IO的缺陷在于，当有大量IO操作时，信号较多，若处理不及时可能导致信号队列溢出；并且内核态与用户态频繁的信号交互也会导致性能较低

## 五、异步IO

在上面的模型中，为了读取一条数据，我们都要分为2步：询问准备状态，就绪后读取数据

异步IO将2步简化为了1步，**应用仅告知内核准备读取数据，并让内核在整个操作完成之后通知应用**。这种模型与信号驱动模型的主要区别在于，**信号驱动IO只是由内核通知我们合适可以开始下一个IO操作，而异步IO模型是由内核通知我们操作什么时候完成**

### 1、Proactor模式

Proactor就是在Reactor的基础上，采用了异步IO模型
